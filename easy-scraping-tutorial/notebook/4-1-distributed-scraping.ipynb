{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们的分布式爬虫\n",
    "首先 import 全部要用的模块, 并规定一个主页. 注意, 我用这份代码测试我内网的网站(速度不受外网影响) 所以使用的 base_url 是 http://127.0.0.1:4000/, 如果你要爬 莫烦Python, 你的 base_url 要是 https://mofanpy.com/ (下载速度会受外网影响)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://192.168.5.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://192.168.5.1:7890\"\n",
    "\n",
    "# base_url = \"http://127.0.0.1:4000/\"\n",
    "base_url = 'https://mofanpy.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "特别注意: 任何网站都是有一个服务器压力的, 如果你爬的过于频繁, 特别是使用多进程爬取或异步爬取, 一次性提交请求给服务器太多次, 这将可能会使得服务器瘫痪, 你可能再也看不到莫烦 Python 了. 所以为了安全起见, 我限制了爬取数量(restricted_crawl=True). 因为我测试使用的是内网 http://127.0.0.1:4000/ 所以不会有这种压力. 你在以后的爬网页中, 会经常遇到这样的爬取次数的限制 (甚至被封号). 我以前爬 github 时就被限制成一小时只能爬60页."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if base_url != \"http://127.0.0.1:4000/\":\n",
    "    restricted_crawl = True\n",
    "else:\n",
    "    restricted_crawl = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义两个功能, 一个是用来爬取网页的(crawl), 一个是解析网页的(parse). 有了前几节内容的铺垫, 你应该能一言看懂下面的代码. crawl() 用 urlopen 来打开网页, 我用的内网测试, 所以为了体现下载网页的延迟, 添加了一个 time.sleep(0.1) 的下载延迟. 返回原始的 HTML 页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "    time.sleep(0.1)             # slightly delay for downloading\n",
    "    return response.read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse() 就是在这个 HTML 页面中找到需要的信息, 我们用 BeautifulSoup 找 (BeautifulSoup 教程). 返回找到的信息."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')})\n",
    "    title = soup.find('h1').get_text().strip()\n",
    "    page_urls = set([urljoin(base_url, url['href']) for url in urls])\n",
    "    url = soup.find('meta', {'property': \"og:url\"})['content']\n",
    "    return title, page_urls, url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试普通爬法\n",
    "\n",
    "为了对比效果, 我们将在下面对比普通的爬虫和这种分布式的效果. 如果是普通爬虫, 我简化了一下接下来的代码, 将一些不影响的代码去除掉了, 如果你想看全部的代码, 请来到我的 Github. 我们用循环一个个 crawl unseen 里面的 url, 爬出来的 HTML 放到 parse 里面去分析得到结果. 接着就是更新 seen 和 unseen 这两个集合了.\n",
    "\n",
    "特别注意: 任何网站都是有一个服务器压力的, 如果你爬的过于频繁, 特别是使用多进程爬取或异步爬取, 一次性提交请求给服务器太多次, 这将可能会使得服务器瘫痪, 你可能再也看不到莫烦 Python 了. 所以为了安全起见, 我限制了爬取数量(restricted_crawl=True). 因为我测试使用的是内网 http://127.0.0.1:4000/ 所以不会有这种压力. 你在以后的爬网页中, 会经常遇到这样的爬取次数的限制 (甚至被封号). 我以前爬 github 时就被限制成一小时只能爬60页."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "网页中爬取中, 肯定会爬到重复的网址, 为了去除掉这些重复, 我们使用 python 的 set 功能. 定义两个 set, 用来搜集爬过的网页和没爬过的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "unseen = set([base_url,])\n",
    "seen = set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributed Crawling...\n",
      "\n",
      "Distributed Parsing...\n",
      "\n",
      "Analysing...\n",
      "1 莫烦Python主页 https://mofanpy.com/\n",
      "\n",
      "Distributed Crawling...\n",
      "\n",
      "Distributed Parsing...\n",
      "\n",
      "Analysing...\n",
      "2 有趣的机器学习 | 莫烦Python https://mofanpy.com/tutorials/machine-learning/ML-intro/\n",
      "Total time: 0.4 s\n"
     ]
    }
   ],
   "source": [
    "count, t1 = 1, time.time()\n",
    "\n",
    "while len(unseen) != 0:                 # still get some url to visit\n",
    "    if restricted_crawl and len(seen) > 20:\n",
    "            break\n",
    "        \n",
    "    print('\\nDistributed Crawling...')\n",
    "    htmls = [crawl(url) for url in unseen]\n",
    "\n",
    "    print('\\nDistributed Parsing...')\n",
    "    results = [parse(html) for html in htmls]\n",
    "\n",
    "    print('\\nAnalysing...')\n",
    "    seen.update(unseen)         # seen the crawled\n",
    "    unseen.clear()              # nothing unseen\n",
    "\n",
    "    for title, page_urls, url in results:\n",
    "        print(count, title, url)\n",
    "        count += 1\n",
    "        unseen.update(page_urls - seen)     # get new url to crawl\n",
    "print('Total time: %.1f s' % (time.time()-t1, ))    # 53 s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这种单线程的方法, 在我的内网上面爬, 爬完整个 莫烦Python, 一共消耗 52.3秒. 接着我们把它改成多进程分布式.\n",
    "## 测试分布式爬法\n",
    "还是上一个 while 循环, 首先我们创建一个进程池(Pool). 不太懂进程池的朋友看过来. 然后我们修改得到 htmls 和 results 的两句代码. 其他都不变, 只将这两个功能给并行了. 我在这里写的都是简化代码, 你可以在这里 看到完整代码."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true,
     "name": "#%% \n"
    }
   },
   "source": [
    "unseen = set([base_url,])\n",
    "seen = set()\n",
    "\n",
    "pool = mp.Pool(4)                       \n",
    "count, t1 = 1, time.time()\n",
    "while len(unseen) != 0:                 # still get some url to visit\n",
    "    if restricted_crawl and len(seen) > 20:\n",
    "            break\n",
    "    print('\\nDistributed Crawling...')\n",
    "    crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen]\n",
    "    htmls = [j.get() for j in crawl_jobs]                                       # request connection\n",
    "\n",
    "    print('\\nDistributed Parsing...')\n",
    "    parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]\n",
    "    results = [j.get() for j in parse_jobs]                                     # parse html\n",
    "\n",
    "    print('\\nAnalysing...')\n",
    "    seen.update(unseen)         # seen the crawled\n",
    "    unseen.clear()              # nothing unseen\n",
    "\n",
    "    for title, page_urls, url in results:\n",
    "        print(count, title, url)\n",
    "        count += 1\n",
    "        unseen.update(page_urls - seen)     # get new url to crawl\n",
    "print('Total time: %.1f s' % (time.time()-t1, ))    # 16 s !!!"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributed Crawling...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "还是在内网测试, 只用了 16.3秒!! 这可比上面的单线程爬虫快了3.5倍. 而且我还不是在外网测试的. 如果在外网, 爬取一张网页的时间更长, 使用多进程会更加有效率, 节省的时间更多.\n",
    "\n",
    "看到这里, 你一定觉得多线程是爬虫的救星. 其实不然, 要不然我们的教程为什么还能继续. 哈哈. 下一节, 我们会讲到比多进程更加厉害的一种方法. 叫做异步爬取 (asyncio 模块).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5d4c1c6c",
   "language": "python",
   "display_name": "PyCharm (tutorials)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}