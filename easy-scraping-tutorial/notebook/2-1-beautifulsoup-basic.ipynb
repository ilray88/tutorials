{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 解析网页: 基础\n",
    "这次我们还是爬一爬上次爬的那个基本网页. BeautifulSoup 使用起来非常简单, 我们先按常规读取网页.\n",
    "回顾一下, 每张网页中, 都有两大块, 一个是 <head>, 一个是 <body>, 我们等会用 BeautifulSoup 来找到 body 中的段落 <p> 和所有链接 <a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Scraping tutorial 1 | 莫烦Python</title>\n",
      "\t<link rel=\"icon\" href=\"{{ static_url }}/js/description/tab_icon.png\">\n",
      "</head>\n",
      "<body>\n",
      "\t<h1>爬虫测试1</h1>\n",
      "\t<p>\n",
      "\t\t这是一个在 <a href=\"/\">莫烦Python</a>\n",
      "\t\t<a href=\"/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# if has Chinese, apply decode()\n",
    "html = urlopen(\"https://mofanpy.com/static/scraping/basic-structure.html\").read().decode('utf-8')\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "读取这个网页信息, 我们将要加载进 BeautifulSoup, 以 lxml 的这种形式加载. 除了 lxml,\n",
    " 其实还有很多形式的解析器, 不过大家都推荐使用 lxml 的形式. 然后 soup 里面就有着这个\n",
    " HTML 的所有信息. 如果你要输出 <h1> 标题, 可以就直接 soup.h1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>爬虫测试1</h1>\n",
      "<title>Scraping tutorial 1 | 莫烦Python</title>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, features='html.parser')\n",
    "print(soup.h1)\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <p>\n",
      "\t\t这是一个在 <a href=\"/\">莫烦Python</a>\n",
      "<a href=\"/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n"
     ]
    }
   ],
   "source": [
    "print('\\n', soup.p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果网页中有过个同样的 tag, 比如链接 <a>, 我们可以使用 find_all() 来找到所有的选项. 因为我们真正的 link 不是在 <a> 中间 </a>, 而是在 <a href=\"link\"> 里面, 也可以看做是 <a> 的一个属性. 我们能用像 Python 字典的形式, 用 key 来读取 l[\"href\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['/', '/tutorials/data-manipulation/scraping/']\n"
     ]
    }
   ],
   "source": [
    "all_href = soup.find_all('a')\n",
    "all_href = [l['href'] for l in all_href]\n",
    "print('\\n', all_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "懂得这些还是远远不够的, 真实情况往往比这些复杂. BeautifulSoup 还有很多其他的选择增强器. 下次, 我们来了解一些 CSS 的概念, 用 BeautifulSoup 加上 CSS 来选择内容.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}