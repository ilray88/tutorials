{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy 爬虫\n",
    "\n",
    "我们导入 scrapy 模块, 并创建一个 spider 的 class. 并继承 scrapy.Spider, 一定还要给这个 spider 一个名字, 我就用 mofan 好了, 因为是爬 莫烦Python 的. 给定一些初始爬取的网页, 写在 start_urls 里. 这里特别要提的是: 之前我们用 python 的 set 来去除重复的 url, 在 scrapy 中, 这是不需要的, 因为它自动帮你去重. 这可省心多了. 如果你想一次性看到全部代码, 请看到我的 github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class MofanSpider(scrapy.Spider):\n",
    "    name = \"mofan\"\n",
    "    start_urls = [\n",
    "        'https://mofanpy.com/',\n",
    "    ]\n",
    "    # unseen = set()\n",
    "    # seen = set()      # we don't need these two as scrapy will deal with them automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "接着我们还要定义这个 class 中的一个功能就能完事了. 我们使用 python 的 yield 来返回搜集到的数据 (为什么是yield? 因为在 scrapy 中也有异步处理, 加速整体效率). 这些 title 和 url 的数据, 我们都是用 scrapy 中抓取信息的方式."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "    def parse(self, response):\n",
    "        yield {     # return some results\n",
    "            'title': response.css('h1::text').extract_first(default='Missing').strip().replace('\"', \"\"),\n",
    "            'url': response.url,\n",
    "        }\n",
    "\n",
    "        urls = response.css('a::attr(href)').re(r'^/.+?/$')     # find all sub urls\n",
    "        for url in urls:\n",
    "            yield response.follow(url, callback=self.parse)     # it will filter duplication automatically\n",
    "\n",
    "\n",
    "# lastly, run this in terminal\n",
    "# scrapy runspider 5-2-scrapy.py -o res.json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在这个response网页中筛选 urls, 这里我们也不需要使用 urljoin() 这种功能给 url 改变形式. 它在 follow() 这一步会自动检测 url 的格式. (真是省心啊~), 然后对于每个找到的 url, 然后 yield 重新使用 self.parse() 来爬取, 这里又是自动去重! Scrapy 仿佛知道你最不想做什么, 它自动帮你都做好了. 开心~\n",
    "\n",
    "最后需要运行的时候有点不同, 你需要在 terminal 或 cmd 中运行这个爬虫. 而且还能帮你保存刚刚 yield 的 {title:, url:} 的结果. runspider 5-2-scrapy.py 就是选择你要跑的这个 Python 文件."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5d4c1c6c",
   "language": "python",
   "display_name": "PyCharm (tutorials)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}